## Title
State Inconsistency and Data Corruption from Non-Atomic Migration Execution Without Pre/Post Validation

## Summary
The in-place store migration system executes migrations directly on the live state without using cached contexts or atomic transactions, and provides no validation of state consistency before or after migrations. When a migration fails partway through execution, the blockchain state becomes corrupted with partially-migrated data while the version tracking remains unchanged, causing the node to re-attempt the same migration on already-modified data upon restart, leading to permanent data corruption and potential chain halt.

## Impact
High

## Finding Description

**Location:** 
- Primary: `x/upgrade/keeper/keeper.go` (ApplyUpgrade function)
- Secondary: `types/module/configurator.go` (runModuleMigrations function)
- Affected: Migration functions such as `x/slashing/keeper/migrations.go` (Migrate2to3, Migrate3to4)

**Intended Logic:**
The migration system is designed to perform in-place store migrations during chain upgrades. The system should ensure that either all migrations complete successfully and state is consistently updated, or no changes are persisted if any migration fails, maintaining state integrity.

**Actual Logic:**
The migration execution flow has critical flaws:

1. In `ApplyUpgrade`, the upgrade handler is called with the actual context (not a cached context): [1](#0-0) 

2. The handler typically calls `RunMigrations`, which executes `runModuleMigrations` for each module: [2](#0-1) 

3. Migrations run sequentially and directly modify the store without any rollback mechanism: [3](#0-2) 

4. The `SetModuleVersionMap` is only called AFTER the handler returns successfully, meaning if a migration fails, the version map is never updated: [4](#0-3) 

5. Complex migrations like `Migrate2to3` perform multiple non-atomic operations: [5](#0-4) 

The migration first writes new signing info (lines 65-68), then deletes old keys (lines 115-125), then writes new missed heights (lines 127-142). If an error occurs at any point after line 65, the state is partially migrated.

**Exploit Scenario:**
This vulnerability is triggered automatically during any chain upgrade with store migrations:

1. A scheduled upgrade reaches its target height
2. BeginBlock executes the upgrade handler, which calls migrations
3. A migration function (e.g., `Migrate2to3`) begins executing and modifies state (writes new signing info for some validators)
4. The migration encounters an error midway through (e.g., unmarshal failure, disk I/O error, out of memory)
5. The handler returns the error, causing `ApplyUpgrade` to panic
6. The node crashes, but the partial state modifications are in deliverState
7. On restart, the node attempts to process the same upgrade height again
8. The `GetModuleVersionMap` still returns the OLD version (because `SetModuleVersionMap` was never called)
9. The migration runs again, attempting to migrate already-migrated validators from v2â†’v3 format
10. This causes unmarshal errors, data corruption, or double-migration of data
11. Different validators may crash at different points, causing consensus failures

**Security Failure:**
This breaks multiple critical security properties:
- **State consistency**: Partial migrations leave the blockchain in an undefined state
- **Determinism**: Different nodes may have different partial migration states
- **Recoverability**: No rollback mechanism exists to restore pre-migration state
- **Consensus agreement**: Nodes cannot reach consensus when state is corrupted

## Impact Explanation

**Affected Assets and Processes:**
- All blockchain state undergoing migration
- Consensus mechanism and validator operations
- Transaction processing and block production
- Critical protocol parameters and account data

**Severity of Damage:**
- **Chain Halt**: The network cannot proceed past the upgrade height due to state corruption
- **Data Corruption**: Validator signing information, account balances, or other critical state may be permanently corrupted with mixed format data
- **Consensus Failure**: Different nodes may have different corrupted states, preventing consensus
- **Irrecoverable State**: Without manual intervention (hard fork or state rollback), the chain cannot recover

**System Impact:**
This vulnerability directly causes "Network not being able to confirm new transactions (total network shutdown)" because:
1. The upgrade triggers automatic migration execution
2. Any migration failure leaves state corrupted
3. Retry attempts compound the corruption
4. The chain cannot produce new blocks until the issue is manually resolved
5. All network participants are affected simultaneously

## Likelihood Explanation

**Who Can Trigger:**
This vulnerability is triggered automatically by the upgrade mechanism itself - no attacker action is required. Any scheduled upgrade that includes store migrations is at risk.

**Required Conditions:**
- A scheduled chain upgrade with in-place store migrations
- Any transient failure during migration execution (disk I/O error, memory pressure, data corruption, unexpected data format)
- Complex migrations that perform multiple store operations

**Frequency:**
- Occurs during every upgrade that has multi-step migrations
- Risk increases with:
  - Large state sizes (more data to migrate)
  - Complex migration logic (more potential failure points)
  - Resource constraints (memory, disk I/O)
  - Number of validators (more data to process in slashing module)

The likelihood is **MEDIUM to HIGH** because:
1. Upgrades are scheduled events that occur regularly
2. Production environments have resource constraints that can cause transient failures
3. Large validator sets and extensive state increase failure probability
4. The vulnerability triggers automatically without requiring attacker action

## Recommendation

Implement atomic migration execution with proper state validation:

1. **Use Cached Context for Migrations**: Wrap the entire migration execution in a cached context that can be discarded on failure:
   ```go
   func (k Keeper) ApplyUpgrade(ctx sdk.Context, plan types.Plan) {
       handler := k.upgradeHandlers[plan.Name]
       if handler == nil {
           panic("ApplyUpgrade should never be called without first checking HasHandler")
       }
       
       // Create cached context for atomic migration
       cachedCtx, writeCache := ctx.CacheContext()
       
       updatedVM, err := handler(cachedCtx, plan, k.GetModuleVersionMap(ctx))
       if err != nil {
           // Don't write cache, discard all migration changes
           panic(err)
       }
       
       // Only write changes if all migrations succeeded
       writeCache()
       k.SetModuleVersionMap(ctx, updatedVM)
       // ... rest of the function
   }
   ```

2. **Add Pre-Migration Validation**: Before executing migrations, validate that the current state matches expected pre-migration conditions (e.g., verify all data is in expected old format).

3. **Add Post-Migration Validation**: After migrations complete, validate that the resulting state is consistent and complete (e.g., verify all validators were migrated, no orphaned data exists).

4. **Implement Migration Checkpoints**: For complex migrations, implement checkpoints and idempotency so migrations can be safely retried from the last successful checkpoint.

5. **Add State Hash Verification**: Store pre-migration state hashes and verify post-migration that only expected changes occurred.

## Proof of Concept

**File:** `x/upgrade/keeper/keeper_test.go`

**Test Function:** Add the following test to demonstrate the vulnerability:

```go
func (s *KeeperTestSuite) TestMigrationFailureLeavesInconsistentState() {
    // Setup: Create a migration that fails partway through
    initialVM := module.VersionMap{"testmodule": uint64(1)}
    s.app.UpgradeKeeper.SetModuleVersionMap(s.ctx, initialVM)
    
    // Track whether state was modified before failure
    stateModified := false
    
    s.app.UpgradeKeeper.SetUpgradeHandler("partial-failure", func(ctx sdk.Context, _ types.Plan, vm module.VersionMap) (module.VersionMap, error) {
        // Simulate a migration that modifies state then fails
        store := ctx.KVStore(s.app.GetKey(types.StoreKey))
        
        // Write some state (simulating partial migration)
        store.Set([]byte("migrated_key_1"), []byte("new_value"))
        stateModified = true
        
        // Simulate failure before completing migration
        return nil, fmt.Errorf("migration failed after modifying state")
    })
    
    plan := types.Plan{
        Name:   "partial-failure",
        Height: 100,
    }
    
    // Attempt upgrade - this should panic due to migration error
    defer func() {
        if r := recover(); r == nil {
            s.T().Error("Expected panic from failed migration")
        }
    }()
    
    s.app.UpgradeKeeper.ApplyUpgrade(s.ctx, plan)
    
    // VULNERABILITY: If we reach here after panic recovery, check state
    // The VersionMap should still show old version
    vmAfterFailure := s.app.UpgradeKeeper.GetModuleVersionMap(s.ctx)
    s.Require().Equal(uint64(1), vmAfterFailure["testmodule"], 
        "VersionMap should not be updated after failed migration")
    
    // But state was modified (proving inconsistency)
    store := s.ctx.KVStore(s.app.GetKey(types.StoreKey))
    s.Require().True(stateModified, "State was modified before failure")
    s.Require().NotNil(store.Get([]byte("migrated_key_1")), 
        "VULNERABILITY: State modifications persist despite migration failure")
    
    // On retry, the migration would run again on already-modified state
    // This would cause data corruption or double-migration
}
```

**Setup:**
1. Initialize upgrade keeper with a test module at version 1
2. Create an upgrade handler that simulates a migration that modifies state and then fails

**Trigger:**
1. Call `ApplyUpgrade` with the plan
2. The migration modifies state (writes a key)
3. The migration then returns an error
4. `ApplyUpgrade` panics due to the error

**Observation:**
The test demonstrates that:
1. State modifications persist in the store even after migration failure
2. The VersionMap is NOT updated (still shows version 1)
3. This creates an inconsistent state where some migration changes are applied but the version tracking indicates no migration occurred
4. On restart/retry, the same migration would run again on partially-migrated data
5. This proves the lack of atomicity and validation in the migration system

The test will show that state changes are visible even after the panic, confirming that migrations do not run in a cached/atomic context and lack proper validation or rollback mechanisms.

### Citations

**File:** x/upgrade/keeper/keeper.go (L365-391)
```go
func (k Keeper) ApplyUpgrade(ctx sdk.Context, plan types.Plan) {
	handler := k.upgradeHandlers[plan.Name]
	if handler == nil {
		panic("ApplyUpgrade should never be called without first checking HasHandler")
	}

	updatedVM, err := handler(ctx, plan, k.GetModuleVersionMap(ctx))
	if err != nil {
		panic(err)
	}

	k.SetModuleVersionMap(ctx, updatedVM)

	// incremement the protocol version and set it in state and baseapp
	nextProtocolVersion := k.getProtocolVersion(ctx) + 1
	k.setProtocolVersion(ctx, nextProtocolVersion)
	if k.versionSetter != nil {
		// set protocol version on BaseApp
		k.versionSetter.SetProtocolVersion(nextProtocolVersion)
	}

	// Must clear IBC state after upgrade is applied as it is stored separately from the upgrade plan.
	// This will prevent resubmission of upgrade msg after upgrade is already completed.
	k.ClearIBCState(ctx, plan.Height)
	k.ClearUpgradePlan(ctx)
	k.SetDone(ctx, plan.Name)
}
```

**File:** types/module/configurator.go (L91-117)
```go
func (c configurator) runModuleMigrations(ctx sdk.Context, moduleName string, fromVersion, toVersion uint64) error {
	// No-op if toVersion is the initial version or if the version is unchanged.
	if toVersion <= 1 || fromVersion == toVersion {
		return nil
	}

	moduleMigrationsMap, found := c.migrations[moduleName]
	if !found {
		return sdkerrors.Wrapf(sdkerrors.ErrNotFound, "no migrations found for module %s", moduleName)
	}

	// Run in-place migrations for the module sequentially until toVersion.
	for i := fromVersion; i < toVersion; i++ {
		migrateFn, found := moduleMigrationsMap[i]
		if !found {
			return sdkerrors.Wrapf(sdkerrors.ErrNotFound, "no migration found for module %s from version %d to version %d", moduleName, i, i+1)
		}
		ctx.Logger().Info(fmt.Sprintf("migrating module %s from version %d to version %d", moduleName, i, i+1))

		err := migrateFn(ctx)
		if err != nil {
			return err
		}
	}

	return nil
}
```

**File:** types/module/module.go (L571-574)
```go
			err := c.runModuleMigrations(ctx, moduleName, fromVersion, toVersion)
			if err != nil {
				return nil, err
			}
```

**File:** x/slashing/keeper/migrations.go (L32-144)
```go
func (m Migrator) Migrate2to3(ctx sdk.Context) error {
	store := ctx.KVStore(m.keeper.storeKey)
	valMissedMap := make(map[string]types.ValidatorMissedBlockArrayLegacyMissedHeights)

	ctx.Logger().Info("Migrating Signing Info")
	signInfoIter := sdk.KVStorePrefixIterator(store, types.ValidatorSigningInfoKeyPrefix)
	newSignInfoKeys := [][]byte{}
	newSignInfoVals := []types.ValidatorSigningInfoLegacyMissedHeights{}
	// Note that we close the iterator twice. 2 iterators cannot be open at the same time due to mutex on the storage
	// This close within defer is a safety net, while the close() after iteration is to close the iterator before opening
	// a new one.
	defer signInfoIter.Close()
	for ; signInfoIter.Valid(); signInfoIter.Next() {
		ctx.Logger().Info(fmt.Sprintf("Migrating Signing Info for key: %v\n", signInfoIter.Key()))
		var oldInfo types.ValidatorSigningInfo
		m.keeper.cdc.MustUnmarshal(signInfoIter.Value(), &oldInfo)

		newInfo := types.ValidatorSigningInfoLegacyMissedHeights{
			Address:             oldInfo.Address,
			StartHeight:         oldInfo.StartHeight,
			JailedUntil:         oldInfo.JailedUntil,
			Tombstoned:          oldInfo.Tombstoned,
			MissedBlocksCounter: oldInfo.MissedBlocksCounter,
		}
		newSignInfoKeys = append(newSignInfoKeys, signInfoIter.Key())
		newSignInfoVals = append(newSignInfoVals, newInfo)
	}
	signInfoIter.Close()

	if len(newSignInfoKeys) != len(newSignInfoVals) {
		return fmt.Errorf("new sign info data length doesn't match up")
	}
	ctx.Logger().Info("Writing New Signing Info")
	for i := range newSignInfoKeys {
		bz := m.keeper.cdc.MustMarshal(&newSignInfoVals[i])
		store.Set(newSignInfoKeys[i], bz)
	}

	ctx.Logger().Info("Migrating Missed Block Bit Array")
	keysToDelete := [][]byte{}
	iter := sdk.KVStorePrefixIterator(store, types.ValidatorMissedBlockBitArrayKeyPrefix)
	// Note that we close the iterator twice. 2 iterators cannot be open at the same time due to mutex on the storage
	// This close within defer is a safety net, while the close() after iteration is to close the iterator before opening
	// a new one.
	defer iter.Close()
	for ; iter.Valid(); iter.Next() {
		// need to use the key to extract validator cons addr
		// last 8 bytes are the index
		// remove the store prefix + length prefix
		key := iter.Key()
		consAddrBytes, indexBytes := key[2:len(key)-8], key[len(key)-8:]

		consAddr := sdk.ConsAddress(consAddrBytes)
		index := int64(binary.LittleEndian.Uint64(indexBytes))
		// load legacy signing info type
		var signInfo types.ValidatorSigningInfoLegacyMissedHeights
		signInfoKey := types.ValidatorSigningInfoKey(consAddr)
		bz := store.Get(signInfoKey)

		m.keeper.cdc.MustUnmarshal(bz, &signInfo)
		// signInfo, found := m.keeper.GetValidatorSigningInfo(ctx, consAddr)
		// if !found {
		// 	return fmt.Errorf("signing info not found")
		// }
		arr, ok := valMissedMap[consAddr.String()]
		if !ok {
			ctx.Logger().Info(fmt.Sprintf("Migrating for next validator with consAddr: %s\n", consAddr.String()))
			arr = types.ValidatorMissedBlockArrayLegacyMissedHeights{
				Address:       consAddr.String(),
				MissedHeights: make([]int64, 0),
			}
		}
		var missed gogotypes.BoolValue
		m.keeper.cdc.MustUnmarshal(iter.Value(), &missed)
		if missed.Value {
			arr.MissedHeights = append(arr.MissedHeights, index+signInfo.StartHeight)
		}

		valMissedMap[consAddr.String()] = arr
		keysToDelete = append(keysToDelete, iter.Key())
	}
	iter.Close()

	ctx.Logger().Info(fmt.Sprintf("Starting deletion of missed bit array keys (total %d)", len(keysToDelete)))
	interval := len(keysToDelete) / 50
	if interval == 0 {
		interval = 1
	}
	for i, key := range keysToDelete {
		store.Delete(key)
		if i%interval == 0 {
			ctx.Logger().Info(fmt.Sprintf("Processing index %d", i))
		}
	}

	ctx.Logger().Info("Writing new validator missed heights")
	valKeys := []string{}
	for key := range valMissedMap {
		valKeys = append(valKeys, key)
	}
	sort.Strings(valKeys)
	for _, key := range valKeys {
		missedBlockArray := valMissedMap[key]
		consAddrKey, err := sdk.ConsAddressFromBech32(key)
		ctx.Logger().Info(fmt.Sprintf("Writing missed heights for validator: %s\n", consAddrKey.String()))
		if err != nil {
			return err
		}
		bz := m.keeper.cdc.MustMarshal(&missedBlockArray)
		store.Set(types.ValidatorMissedBlockBitArrayKey(consAddrKey), bz)
	}
	ctx.Logger().Info("Done migrating")
	return nil
```
